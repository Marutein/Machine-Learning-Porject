---
title: "Machine Learning Final Project"
subtitle: "Predicting Orange Juice Purchase"
author: "Gary Buckley and Ivan Espino"
output: 
  html_document:
    number_sections: TRUE
date: "2022-11-16"
---

# Problem Statement

The Orange Juice Category presents a significant opportunity for our firm. We sell two brands of orange juice to our customers: Citrus Hill (CH) and Minute Maid (MM). Our company makes higher margins on MM orange juice than CH, so the higher percentage of customers that buy MM, the more profit we will make.

How do we know who will buy MM orange juice? There are two layers to this question. First, we need to explore what factors cause (or are at least correlated with) customers to buy MM over CH. Answering this question allows us to target our advertising to these customers, position our brand to attract these types of buyers, and other possible strategies to improve our margin in the orange juice category. 

The second question is can we predict who will buy MM over CH? This is similar to question 1, but goes a bit further. Question 1 is a descriptive analysis - figuring our what influenced buyers to purchase MM in the past. This second question is a predictive analysis: if a new customer walks into our store, we want to predict how likely they are to purchase MM. The results from question 1 and 2 work together. If we know what factors influence people to buy MM, we can adjust our strategy accordingly. Then, if we can predict purchase behavior, we can predict the influence of the new strategy on our revenue. This allows us to run a cost-benefit analysis and determine if the strategy is worthwhile.

To answer these questions, we have collected 1070 purchases in which the customer purchased orange juice. Note that we are limiting ourselves to the population of consumsers that buy orange juice. We are NOT finding what causes a customer to buy or not buy orange juice. Instead, we are finding what causes a customer who is purchasing orange juice to choose MM over CH. 

As a result of our analysis, the brand manager can expect us to identify the factors that most strongly influence whether a customer purchases MM or not. In addition, the sales manager can expect a model that will predict whether a customer will purchase MM or CH along with the expected performance of the model.

# Methods

## Logistic Regression

We begin by building a logistic regression model. The advantage of regression models is that they clearly measure the impact of a factor on our target variable. We run a logistic regression since our target variable is binary. This model will help us evaluate which factors are the most important influencers of purchase behavior. 

```{r, warning=FALSE,library}
#Libraries used
library(dplyr)
library(plotROC)
library(ggplot2)
library(caret)
library(performance)
library(car)
library(ROCR)
library(glmnet)
library(corrplot)
library(AICcmodavg)
library(plotROC)
```

### Data Preparation

We start by pulling and preparing our data

```{r, data}
#Data set
df<-read.csv(url("http://data.mishra.us/files/project/OJ_data.csv"))
#ensure all predictors are numeric
df[2:14] <- lapply(df[2:14], as.numeric)
#code target as factor since it is binary
df$Purchase <- as.factor(df$Purchase)
glimpse(df)
head(df)
```

#### Code Target Variable

The data is coded such that CH is labeled as 1 and MM as 0. We will swtich these since we want to model whether the customer purchased MM.

```{r, refactor target}
#Switching Purchase values: 0 to 1 = MM Purchased and 1 to 0 = MM No Purchased.
#A factor with levels 0 and 1 indicating whether the customer purchased Citrus Hill (1) or Minute Maid Orange Juice (0).

df <- df %>%
  mutate(across(Purchase, ~ case_when(. == 0 ~ "Yes",
                                        . == 1 ~ "No",
                                        TRUE ~ NA_character_)))
df <- df %>%
      mutate(Purchase = ifelse(Purchase == "No",0,1))

head(df)
```

#### Split train and test

We'd like to evaluate the predictive power of this model. To do so, we must hold out some of the data so we can test our model on it. We will keep 70% of our data to train the model and use 30% to test it.

```{r, logistic split}
#Splitting the data into Train, Test
split = 0.7
set.seed(1234)

train.data <- sample(1:nrow(df), split * nrow(df))
test.data <- setdiff(1:nrow(df), train.data)

train.data <- df[train.data,]
test.data <- df[test.data,]
```

#### Multicollinearity

To ensure our model runs as well as possible, we need to process our predictor variables with some tests and checks.

```{r, logistic predictors}
#split predictors and target
predictors <- train.data[,c(-1)]
purchase <- train.data$Purchase
```

Our first check is whether our variables are collinear. In other words, if as one predictor goes up another also goes up, they are encoding the same information. We don't want to 'double count' this influence, so we should only use one. (There are other reasons to drop one of the variables too - it just overall helps the model run better).

Below you see a chart of our collinearity. Big dark dots represent high collinearity that we need to do something about. 

```{r, multicorrelation}
# Checking for multicollinearity and plotting correlations between predictors
correlation = cor(train.data)
#correlation
corrplot.mixed(correlation, lower.col = "black", tl.cex=.7,number.cex = .7)
```

##### Perfect collinearity

This our baseline model *without* removing the collinearity. Notice we get some NA values, meaning the predictors are perfecty correlated (correlation = 1 or -1) with others in the model. We need to remove the predictors.  

```{r, logistic baseline}
#Logistic Model
predictionModel1 <- glm(Purchase ~ ., data = train.data,family=binomial(link='logit'))
summary(predictionModel1)
```

We drop the variables and our NA values go away.

```{r, logistic model 2}
#Dropping predictors variables perfectly correlated
drop <- c("SalePriceCH", "SalePriceMM", "ListPriceDiff","PriceDiff")
train.data = train.data[,!(names(train.data) %in% drop)]

predictionModel <- glm(Purchase ~ ., data = train.data, family=binomial(link='logit'))
summary(predictionModel)

```

##### High Multicollinearity: Penalized Regression

We have dropped the perfectly collinear variables, but we'd still like to remove those with high colliearity as well. To do so, we run a test to measure collinearity. High numbers mean high collinearity.

```{r, logistic VIF}
# Calculating VIF for each of the predictors
vif(predictionModel)
```

We have values over 5, meaning we still have high collinearity. To address these, we will use a generalized linear regression technique that automatically selects which variables to exclude. The technique requires we choose a parameter to help us; the plot is showing us which value to choose for our parameter. 

```{r, lasso}
#Examining predictors quality using LASSO
predictors <-data.matrix(predictors)
set.seed(1234)

cv.binomial <- cv.glmnet(x = predictors, y = purchase,
alpha=1, family="binomial",
nfolds=4, standardize = TRUE, type.measure = "auc")
plot(cv.binomial)

#AUC = over 90% of the time the model will classify YES and NO correctly. 
```

The dots tell us we also need to drop these variables to avoid multicollinearity.

```{r, lasso 2}
#Applying shrinkage parameter λ
(best.lambda <- cv.binomial$lambda.min)
y4<- coef(cv.binomial, s="lambda.min", exact=FALSE)
print(y4)
```

### FIXME

I don't think we need this?? This is the result of the penalize regression, which I don't think we need?

```{r}
#Optimal value of λ in the test data
test.predictors <- data.matrix(test.data[,c(-1)])
actual <- test.data$Purchase
pred.model = predict(cv.binomial, newx = test.predictors, type = "response",s ="lambda.min")

pred <- prediction(pred.model, test.data$Purchase)
perf <- performance(pred,"tpr","fpr")
auc_ROCR <- performance(pred,measure ="auc")

# plot ROC curve
plot(perf,colorize=FALSE, col="black") 
lines(c(0,1),c(0,1),col = "gray", lty = 4 )
text(1,0.15,labels=paste("AUC = ",round(auc_ROCR@y.values[[1]],
                                        digits=2),sep=""),adj=1)
```

We now drop all the columns that were causing issues to get our final dataset. 

```{r, final dataset}
#new DF excluding betas less important
drop <- c("PriceCH","PriceMM", "DiscCH", "DiscMM", "SpecialCH", "SalePriceMM","PctDiscMM","ListPriceDiff")
new.df = df[,!(names(df) %in% drop)]
#creating a new DF 
head(new.df)
```

### Final Model

#### Split test and train

We again split our final dataset so we can test the performance of our model

```{r, final dataset split}
#Splitting data
split = 0.7
set.seed(1234)

train_index <- sample(1:nrow(new.df), split * nrow(new.df))
test_index <- setdiff(1:nrow(new.df), train_index)

train_data <- new.df[train_index,]
test_data <- new.df[test_index,]
```

#### Build Model

We can now build the model. We are building a logistic regression to predict the purchase of MM. Once we build the model, we will use it on our test set (the data we held out to test performance) to predict how well our model wil perform in real life. 

```{r, building model}
#Learning relationship between predictor variables and the outcome variables using train data set
predictionModel <- glm(Purchase ~ ., data = train_data,family=binomial(link='logit'))

#Predicting putcome in test data set
test_data$prediction <- predict(predictionModel,newdata = test_data, type ="response")

#Converting probabilities YES and NO into a binary class 50% of probability.
#If predicted probability is 50% or less we classify that prediction as “no: will not purchase the product”. 
#Above 50% probabilities are classified as "yes: will purchase the product".
test_data$binary_prediction<-ifelse(test_data$prediction > 0.5,1,0)
```

#### Results and Evaluation

Accuracy is the percentage of observations we predited correctly in our test set. We can see our model is 82% accurate. 

```{r, logistic accuracy}
#Model Accuracy
test_data <- test_data %>% 
  mutate(accurate = 1*(binary_prediction == test_data$Purchase))

accuracy <- sum(test_data$accurate)/nrow(test_data)
print (paste("Accuracy:",round(accuracy,3)))
```

The confusion matrix gives us a bit more detail. Remember 1 is MM. So the top left cell means we correctly predicted CH and the bottom right means we correctly predicted MM. The top right is when our model predcited MM when in reality the customer purchased CH. The bottom left is our model predicting CH when it should have predicted MM.

```{r, logistic confusion matrix}
#Converting Values to a factors
test_data$binary_prediction<-as.factor(test_data$binary_prediction)
test_data$Purchase<-as.factor(test_data$Purchase)

#Confusion Matrix table
t(confusionMatrix(test_data$binary_prediction,test_data$Purchase)$table)
```

A few more values to analyze our model performance.

```{r, logistic accuracy stats}
#Precision and Recall values
confusionMatrix(test_data$binary_prediction,test_data$Purchase)$byClass
```

An ROC plot helps us determine model performance. We want a plot that goes straight up as much as possible to 1 on the y-axis and then over to right side of the x-axis. 

```{r, logistic ROC}
#ROC Curve
roc_d <- as.data.frame(cbind(test_data$prediction,test_data$Purchase))
basicplot <- ggplot(roc_d, aes(d = V2, m = V1)) + geom_roc(n.cuts = 8, labelsize = 4)
styledplot <- basicplot +
style_roc(xlab = "False Positive Rate", ylab ="True Positive Rate")
styledplot
```

### Logistic Model Conclusion

The accuracy stats above mean we can feel pretty confident in our model. Our accuracy is not too high (meaning we won't be able to predict new cases very well), but also not so low it is not giving us any information. Let's take a final look at our model.

```{r, logistic summary}
summary(predictionModel)
```

We can conclude that two factors are most important in determining whether a customer purchases MM: lolayCH and PriceDiff. Past purchasing behavior is a good indicator of future purchasing behavior. Additionally, the higher the MM is priced above CH, the less likely customers are to buy it. We will explore these results further in the conclusion.

## Gradient Boosted Tree

Above, we built a logistic model that predicted orange juice purchase behavior with 82% accuracy. We now implement a different model - a gradient boosted decision tree - to see if we can improve our accuracy. The downside of this model is that it does not allow us to measure the influence of each predictor on the outcome. To overcome this, we will use some Explainable AI (XAI) methods to see how variables influence the model.  

```{r, warning=FALSE, xgboost libraries}
#librarires used
library(tidymodels)
library(xgboost)
library(vip)
library(DALEXtra)
```

### Data Preparation

We recast our target variable to be a factor. We then split our data into test and train so we can evaluate how our model will do on out-of-sample data. Again, we keep 70% of our data to train the model.

```{r, xgboost split}
#Preparing Data
set.seed(1234)

#recast target as factor
df$Purchase<-as.factor(df$Purchase)

df$Purchase <- factor(df$Purchase)

#split test and train
data_testtrn <- initial_split(df, prop = 0.7, strata = Purchase)

train.data.GBT <- training(data_testtrn)
test.data.GBT <- testing(data_testtrn)

train.data.GBT$Purchase<-as.factor(train.data.GBT$Purchase)
test.data.GBT$Purchase<-as.factor(test.data.GBT$Purchase)
data_testtrn$Purchase<-as.factor(data_testtrn$Purchase)
```

### Final Model

#### Formulate Model

We wil use all variables to estimate whether a customer will purchase MM. Unlike in logistic regression, we do not need to remove correlated variables while implementing xgboost.


```{r, formulation}
#Model Formulation
rec_purchase <- recipe(Purchase~., train.data.GBT)%>%
  prep(training = train.data.GBT)
rec_purchase
```

#### Model type

We will use the xgboost method while building our model. This model requires that we select some hyperparameters: the number of trees to grow to produce an estimate, the depth of each tree, and how quickly the algorithm converges (learning rate). To choose these, we will use a tune grid with cross-validation. 

```{r, method}
#Algorithm Type
model_purchase <- boost_tree(
                     trees = tune(),
                     tree_depth = tune(),
                     learn_rate = tune()) %>% 
                     set_engine("xgboost", verbosity = 0) %>% 
                     set_mode("classification")
```

#### Tuning grid

For the three hyperparameters mentioned above, we will check four levels for each: a total of 64 different iterations of the model will be created and cross-validated. 

```{r, tune}
#Tuning Hyper parameters
hyper_grid <- grid_regular(
  trees(),
  tree_depth(),
  learn_rate(),
  levels = 4)

#Cross-validation vfold_cv
purchase_folds <- vfold_cv(train.data.GBT, v=5)
```

#### Workflow 

We put our formulation, tuning grid, and model type together into a workflow

```{r, workflow}
#Aggregating Information to fit the Model
purchase_wf <- workflow() %>%
  add_model(model_purchase) %>%
  add_recipe(rec_purchase)
```

#### Tune the model

```{r, tune model}
#Performing Metrics to get the best model
#doParallel::registerDoParallel(cores = 10)

#build model
set.seed(1234)
purchase_tune <- purchase_wf %>% 
  tune_grid(
    resamples = purchase_folds,
    grid = hyper_grid,
    metrics = metric_set(roc_auc)
  ) 
```

#### Results and Evaluation

#### Best Model

The best model (the mdoel with the highest AUC value) uses 667 trees each with a depth of 1. It uses a learning rate of 0.1.

```{r, best model}
#using auc, show best five models
show_best(purchase_tune, metric = "roc_auc", n = 5)

#save best model
best_model <- purchase_tune %>%
  select_best("roc_auc")
```

Add this model to the workflow

```{r, workflow 2}
#Updating Workflow
final_workflow <- 
  purchase_wf %>% 
  finalize_workflow(best_model)
```

#### Out-of-Sample metrics

Our accuracy has improved to 84% using this model. It performs slightly better than the logistic model. 

```{r, xgboost accuarcy}
#Validate performance on the test data
final_fit <- final_workflow %>%
    last_fit(split = data_testtrn) 

final_fit %>%
  collect_metrics()
```

#### Variable importance

This plot show the importance of each variable in determining the prediciton of the model. We can see that, again, past purchasing behavior (LoyalCH) and the difference in price between CH and MM (PriceDiff) are the two most infuenctial factors. We also find that the discount offered on CH (DiscCH) also has a mild effect.

```{r, variable importance}
#Plotting which variables played an important role
final_workflow %>%
  fit(data = train.data.GBT) %>%
  extract_fit_parsnip() %>%
  vip(geom = "point")
```

How are these influential factors correalted with outcome? To understand this, we will use PDP plots - a form of XAI. 

From the first plot we see that as LocalCH gets higher (the customer is more loyal to the CH brand) the probabiliyt of buying MM decreases. 

We observe a similar negative trend in the plot for PriceDiff - the great the price difference between MM and CH (MM priced higher) the less likely the customer is to buy MM.

Similarly, the greater the discount on CH, the less likely the customer is to buy MM.

```{r, pdp plot}
#Plot Partial Dependency
model_fitted <- final_workflow %>%
  fit(data = train.data.GBT)
explainer_rf <- explain_tidymodels(model_fitted, 
                                   data = train.data.GBT[,c(-1)],
                                   y = train.data.GBT$Purchase, 
                                   type = "pdp",verbose = FALSE)

pdp_LoyalCH <- model_profile(explainer_rf,
                             variables = "LoyalCH", N=NULL)
pdp_PriceDiff <- model_profile(explainer_rf,
                             variables = "PriceDiff", N=NULL)
pdp_DiscCH <- model_profile(explainer_rf,
                             variables = "DiscCH", N=NULL)

plot(pdp_LoyalCH)
plot(pdp_PriceDiff)
plot(pdp_DiscCH)
```

### XGBoost Model Conclusion

The xgboost model performs slightly better than the logistic model in predicting customer purchase behavior. Using XAI, we confirm that purchase history and price difference are the most influential factors on which product the customer purchases. 

# Results and Conclusion

## To the Brand Manager

Of the factors measured, only five influence whether a customer purchases MM: SpecialMM, LoyalCH, SalePriceCH, PriceDiff, and PctDiscCH. Of these, LoyalCH (the customer's purchase history) and PriceDiff (the difference in price between MM and CH) have the most influence. Customers that buy CH in the past tend to continue buying it in the future. The more expensive MM is than CH, the less likely customers are to buy it. These results were confirmed by both models we run, so we are quite confident these are the two most important factors. 

If you can convert CH customers to buy MM, they will be more likely to purchase this brand in the future. It appears customers are responsive to price. Thus, we suggest lowering the price of MM to below CH for a month. We expect this to convince several usual CH customers to try MM. Once customers have converted, they will be more likely to buy MM in the future, even if the pricee of MM goes up.

## To the Sales Manager

We expect that we can predict whether a customer will purchase MM with 84% accuracy. The model we selected is an xgboost model which we fine-tuned to get the best accuracy. Although this accuracy might be lower than expected, we are confident our model didn't learn *too* well, which would mean it would perform poorly on future predictions. Instead, we are quite confident future predictions will be accurate around 84% of the time.



